\chapter{Conclusions and Future Work}\label{conclusion chapter}

In this thesis project, we focus on the research problem of automatically predicting experimental metadata using scientific publications. This is done to tackle the problem of poor quality metadata. To make data reusable we need to assess the quality of metadata, and we hypothesize that using scientific publications for prediction of experimental metadata would help with increasing this quality. We also formalize five research questions toward answering this vast research problem. 

In chapter~\ref{chap2} we discussed the literature survey - related work done close to this research problem. In chapter~\ref{chap:method} provided a brief description of the methods used and a description of the dataset. In chapter~\ref{chap:nnmethod} we discussed the model specification of CNN and BiLSTM in the context of NLP. Lastly in Chapter~\ref{chap: results} we presented an analysis of the results. In this chapter~\ref{conclusion chapter} we discuss the conclusion of this work (Section~\ref{sec:conclusion}), the limitations and future work are listed in Section~\ref{sec:futurework}. 


%We formulate the research questions. 
%\begin{itemize}
%    \item RQ2: Which specific metadata keys (e.g. disease, age, organism) can be accurately predicted?
 %   \item RQ3: What are the optimal set of features that should be selected for the machine learning algorithm to accurately predict metadata fields or the prediction is better without employing features at all?
  %  \item RQ4: Which is the best automated (machine learning/deep learning) method to predict metadata?
   % \item RQ5: To what extent does the accuracy improves when we use abstract and full text?
%\end{itemize}

\section{Conclusions}\label{sec:conclusion}
We started by looking for annotated datasets, which use standard vocabulary so that we can train a model with that dataset. We started with the metadata key ``disease'', as we found a corpus - NCBI disease corpus (refer Section~\ref{section:data}). We trained this data on mainly used two architectures - CNN and BiLSTM. We perform two different kinds of experiments with these two architectures (i) first we predict the disease names by using their unique ID in the MeSH ontology and (ii) second, we use the tree structure of MeSH ontology to move up in the hierarchy of these disease terms which reduces the number of labels. We also perform various multi-label classification techniques for the above mentioned two experiments. We used two different feature extraction techniques for these baseline classifiers - (i) TF-IDF and (ii) Bag of Words. Additionally, we also perform a baseline method of direct matching the disease terms in the text. 
We select the metadata key \emph{disease} for prediction as it is very essential and relevant in the biomedical domain. 

We used the NCBI disease corpus, to predict the disease names using 793 PubMed abstracts. We used GLoVE word embedding to train the CNN and BiLSTM. 
Overall, the CNN model performed the best among all the methods that were used for prediction. Now we answer all the research questions that were outlined in Chapter~\ref{chap:intro}.
\begin{itemize}
    \item RQ1: Up to what extent can we predict metadata from scientific publications?
    \item RQ2: Which specific metadata fields can be accurately be predicted?
    \item RQ3: What are the optimal set of features that should be selected for the machine learning algorithm to accurately predict metadata fields or the prediction is better without employing features at all?
    \item RQ4: Which is the best automated (machine learning/deep learning) method to predict metadata?
    \item RQ5: To what extent does the accuracy improves when we use abstract and full text?
\end{itemize}

\subsection{Research Question 1 \& 2}
In this project, we only look at the prediction of metadata key type `disease'. We performed two different types of experiments (i) predicting disease names and (ii) predicting superclasses of disease names (refer Chapter~\ref{chap: results}). In the first experiment we found that CNN performed best with $\approx$30\% accuracy, but when it comes to predicting the superclasses CNN gave a very high accuracy of $\approx$80\%. This is a promising result, which can be employed when we try to predict more than one metadata key type. 

Since we used a limited number (793) of abstracts therefore, to predict the disease names we did not get a high accuracy in the first experiment. For predicting a large number of labels we need a higher number of abstracts for the accuracy to improve. 
%With a larger dataset, the model sees would be able to see a larger set of examples while training which in-turn would provide better results.

\subsection{Research Question 3 \& 4}
We used baseline multi-label classification with two different feature extraction techniques - a bag of words and TF-IDF. We saw that the TF-IDF feature extraction technique worked better when predicting the disease terms, but the bag of words technique worked better when predicting superclasses (refer Chapter~\ref{chap: results}). 
%The results might improve if we did some more in-depth feature engineering, but even with that, it is unlikely that the performance exceeds the CNN.

From the results, we saw that rather than following the traditional methods of extracting features in the dataset and then select a machine learning model, if we use a pre-trained word embedding we get better results. In this case, we used GloVe: Global Vectors for Word Representation~\cite{pennington2014glove}. We use a pre-trained word embedding because it contains global information about a word and training a word embedding from scratch requires a large amount of data. 

\subsection{Research Question 5}
In this project, we did not include the full-text articles when performing experiments. In the biomedical field, a lot of articles hide behind paycheck walls. One would need to filter out the full access articles which would be very time consuming.  
\\

In conclusion, we saw that traditional methods of performing feature engineering do not perform best when it comes to natural language processing. We would need a numerous number of rules to cover all the ambiguities and exceptions in the language. 
%For example: Phonetics and Phonology - ``I scream" vs. ``ice cream", Morphology -- unionized = ``union” + ``ized” or ``un” + ``ionized”, Discourse -- Merck \& Co. formed a joint venture with Ache Group, of Brazil. \textbf{It} will be called Prodome Ltd.~\footnote{Examples from \url{http://www.cs.cornell.edu/courses/cs674/2003sp/history-4up.pdf}}. 
Additionally, we would always need a different set of rules when dealing with data from different domains, which wouldn't be a trivial task. These methods require more time, cost and a lot of manual effort. 

On the contrary, when we use deep learning we don't have to provide feature specifically, the model learns the features on its own. Therefore, using pre-trained word embeddings give the idea of a language model to the network, this is where it also learns context and ambiguities. Hence when we want to predict various metadata keys in the future, a deep learning model would be a promising technique to move further. 

\section{Limitations and Future Work}\label{sec:futurework}
A major limitation to this problem is the availability of a large gold standard dataset on which a model could be trained. We need a large annotated dataset which could be used to train a model which could then be used to predict metadata keys and values. 
The corpus that was used for this project had a limited amount of abstracts with only disease mentions that were annotated - which is the reason the poor results for predicting disease terms. 
%In this project we used a limited amount of data to train the model, even though we got decent results for superclass prediction, but this specifically could not be generalized for other metadata key types unless we find a similar annotated dataset for other key types. 

As a part of future work, we can extend our model to predict for other metadata key types like an organism, tissue, cell line etc. A more context specific word embedding could be used in the deep learning architecture that we used to check whether this would help improve the predictions of the metadata. Transfer learning could be used - ``Transfer learning and domain adaptation refer to the situation where what has been learned in one setting - is exploited to improve generalization in another setting~\cite{Goodfellow-et-al-2016}''. Moreover, in order to use transfer learning, we train the model on a large annotated corpus like the data present in BioASQ challenge\footnote{\url{http://bioasq.org/participate/challenges}} and test in on the NCBI disease corpus. 







