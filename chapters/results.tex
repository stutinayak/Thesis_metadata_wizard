\chapter{Results and Discussion}\label{chap: results}

In this thesis, the research problem that we are solving is to automatically predict experimental metadata using scientific publications. For such a problem we dive into the realm of text classification because we use scientific publications (unstructured text). As mentioned in Chapter~\ref{chap:nnmethod} for classification in the text a set of labels/categories are required. Therefore, for this purpose, we used the NCBI disease corpus (explained in Section~\ref{section:data}) which contains a set of publications which has annotated disease mentions. Here, we only predict this metadata category disease, as it is a relevant term in the biomedical domain. 

We first start this chapter by defining the evaluation metrics that have been presented in the results. Firstly these metrics are defined for binary classification and then using an example the metrics are defined for multi-label classification. 

\addcontentsline{toc}{section}{Performance measures}
\section*{Performance measures}
For evaluating results from classification, we need measures to assess the performance of the method. Hence, for this purpose statistical measures are used based on the confusion matrix shown in Table~\ref{tab:confusionmatrix}. In the confusion matrix, TP is the number of times the positive class (1) is classified correctly, FP means the number of times the negative class (0) is misclassified as positive (1), TN means the number of times the negative class is classified correctly and FN is the number of times the positive class (1) is classified incorrectly. 

\begin{table}[!htb]
    \centering
    \begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{}} & \multicolumn{2}{c|}{Predicted} \\ \cline{3-4} 
\multicolumn{2}{|c|}{} & 0 & 1 \\ \hline
\multirow{2}{*}{Actual} & 0 & true negative (TN) & false positive (FP) \\ \cline{2-4} 
 & 1 & false negative (FN) & true positive (TP) \\ \hline
\end{tabular}
    \caption{Confusion Matrix - table that is used to calculate evaluation metrics}
    \label{tab:confusionmatrix}
\end{table}

The metrics that have been used in this project are defined as follows: 
\begin{itemize}
    \item Accuracy = $\frac{TP+TN}{TP + TN + FP + FN}$, which is the percentage of times where the model gives correct results. 
    \item Precision = $\frac{TP}{TP + FP}$, this is also known as the positive predictive value. This metric tells the percentage of relevant results among the predicted results. 
    \item Recall = $\frac{TP}{TP + FN}$, also known as sensitivity. This is the percentage of positive instances classified correctly among all the correctly classified instances. 
    \item F1 score = $\frac{2*precision*recall}{precision + recall}$, this is used when we have a class which has a smaller number of occurrences. It helps in combining the trade-offs of precision and recall. 
\end{itemize}

Now we define the metrics in terms of multi-label classification. For this purpose we use an example demonstrated in Table~\ref{tab:multilabelmetrics}:

\begin{table}[!htb]
    \centering
    \begin{tabular}{|l|l|l|}
\hline
Input & $y^{(i)}$ (Actual label) & $\hat{y}^{(i)}$ (Predicted Labels) \\ \hline
$\tilde{x}^{(1)}$ & [1 0 1 0] & [1 0 0 1] \\ \hline
$\tilde{x}^{(2)}$ & [0 1 0 1] & [0 1 0 1] \\ \hline
$\tilde{x}^{(3)}$ & [1 0 0 1] & [1 0 0 1] \\ \hline
$\tilde{x}^{(4)}$ & [0 1 1 0] & [0 1 0 0] \\ \hline
$\tilde{x}^{(5)}$ & [1 0 0 0] & [1 0 0 1] \\ \hline
\end{tabular}
    \caption{An example of predictions in a multi-label classification to depict evaluation metrics}
    \label{tab:multilabelmetrics}
\end{table}

We start by defining the accuracy:
    \begin{equation}
      Accuracy = \frac{1}{N}\sum_{i=1}^N\frac{|\hat{y}^{(i)} \wedge y^{i}|}{|\hat{y}^{(i)} \vee y^{i}|}
    \end{equation}    
where $\wedge$ and $\vee$ are logical OR and AND operations, which are applied vector-wise. 
Then we define the F$_1$ measure for the multi-label classification:
\begin{equation}
    F_1 = \frac{1}{N}\sum_{i=1}^N\frac{2|\hat{y}^{(i)} \wedge y^{i}|}{|\hat{y}^{(i)}| + |y^{i}|}
\end{equation}
Overall, higher the value of accuracy and F$_1$ score the better the learning algorithm. 

We start by presenting a baseline approach to this research problem. Then we move on to the comparison of results (judging mainly by the accuracy). The results are presented using the neural network model and they are compared with baseline multi-label classification methods. The comparison is divided into two parts (i) predicting the disease terms and (ii) predicting the superclass of the disease terms.

The dataset was tested with three types of problem transformations (see Section~\ref{MLtranform}): (i) Binary Relevance (BR), (ii) Label Powerset (LP) and (iii) Classifier Chains (CC). The problem transformations were used from scikit-multilearn that is a library specific to multi-label classification built on top of the well-known scikit-learn.~\cite{2017arXiv171100046S}. The classifiers that were tested using these transformations were 
\begin{itemize}
    \item Multi-layer Perceptron classifier (MLP classifier) 
    \item Multi-label k Nearest Neighbour (MLkNN) 
    \item Random Forest Classifier 
    \item Decision Trees Classifier 
\end{itemize}
For each combination the evaluation metrics used were: accuracy and weighted F1 score. The classifiers used and the metrics were calculated using scikit-learn~\cite{scikit-learn}. 

For the neural network approach, we utilized the pretrained word embeddings - GloVe: Global Vectors for Word Representation~\cite{pennington2014glove} of dimension 300. For data preprocessing we used tokenizer API from Keras~\cite{chollet2015keras}. The CNN and BiLSTM architectures were used from PyTorch\footnote{\url{https://pytorch.org/}}.
For validating the results using the neural networks we split the data into training and validation sets. The training set is 80\% of the whole data and the validation set is the rest 20\%. A similar train-test split was done for the baseline multi-label classification 80\%-20\%. The dropout rate has been kept $0.2$ in all the neural network models. 

\section{A baseline approach}
In the prediction of metadata, specifically diseases - a baseline approach would be to use a vocabulary of disease names and perform a basic search in the documents. If we have a match it can be said that a particular publication talks about `xyz' disease. This would be a direct and trivial way to approach this problem. 

In this case, the disease names from MeSH were downloaded\footnote{\url{http://bioportal.bioontology.org/ontologies/MESH/?p=classes&conceptid=root}}. After that, direct matching is done using Flashtext~\cite{2017arXiv171100046S}. 
This also includes the synonyms so that the matching is not biased for terms like `heart disease' and `cardiovascular disease'. For example in the abstract text heart disease is mentioned but in the list of the downloaded disease terms, cardiovascular diseases are present.  Even after including the synonyms we could find only $44$ unique terms in the text, which is 7.5\% of the identified disease terms (588).

\section{Predicting Disease Terms}
For prediction of disease terms, the unique IDs (explained in Section~\ref{section:data}) are used as labels after preprocessing - removing duplicates and characters if present. 

\subsection{Multi-label Classification}
For baseline multi-label classification two types of feature extraction techniques were used: (i) TF-IDF and (ii) Bag of words representation (BoW). The results for these two techniques are presented in Tables~\ref{tab:resultdiseasetfidf} and~\ref{tab:resultdiseasecount}. For both the techniques the maximum feature length is kept at 200 (reason explain in Section~\ref{sec:discussion}). It can be seen that the MLP Classifier performs the best in both the techniques of feature extraction. 

There is a difference in the performance if we compare the different feature extraction methods, the TF-IDF technique performs better than BoW. The TF-IDF normalizes the outputs which is why this technique is known to perform better when comes to the feature selection, it can be observed from the results as well. 
Since dataset has a large number of diseases - $588$, and a limited number of abstracts, the accuracy of predictions is not very high. The highest accuracy that we get here is $\approx 19\%$. 

\begin{table}[!htb]
    \centering
    \resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
Classifiers & \multicolumn{3}{c|}{Random forest} & \multicolumn{3}{c|}{Decision Tree} & \multicolumn{3}{c|}{MLPClassifier} & \multicolumn{4}{c|}{MLkNN} \\ \hline
\begin{tabular}[c]{@{}l@{}}Problem \\ Transformations\end{tabular} & \multicolumn{1}{c|}{BR} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c|}{BR} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c|}{BR} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c|}{k=20} & \multicolumn{1}{c|}{k=30} & \multicolumn{1}{c|}{k=10} & \multicolumn{1}{c|}{k=5} \\ \hline
\multicolumn{1}{|c|}{Accuracy} & \textbf{0.101} & 0.075 & 0.088 & 0.044 & \textbf{0.107} & 0.082 & 0.170 & \textbf{0.189} & 0.164 & 0.164 & 0.138 & \textbf{0.176} & \textbf{0.176} \\ \hline
\multicolumn{1}{|c|}{F1 score} & 0.125 & 0.191 & 0.117 & 0.259 & 0.208 & 0.250 & 0.207 & 0.233 & 0.203 & 0.194 & 0.184 & 0.238 & 0.232 \\ \hline
\end{tabular}
}
    \caption{Results for Disease terms using TF-IDF feature selection}
    \label{tab:resultdiseasetfidf}
\end{table}

\begin{table}[!htb]
    \centering
    \resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Classifiers & \multicolumn{3}{c|}{Random forest} & \multicolumn{3}{c|}{Decision Tree} & \multicolumn{3}{c|}{MLPClassifier} & \multicolumn{4}{c|}{MLkNN} \\ \hline
\begin{tabular}[c]{@{}c@{}}Problem \\ transformations\end{tabular} & BR & LP & CC & BR & LP & CC & BR & LP & CC & k=20 & k=30 & k=10 & k=5 \\ \hline
Accuracy & 0.088 & \textbf{0.126} & 0.088 & 0.082 & 0.094 & \textbf{0.113} & 0.132 & \textbf{0.157} & 0.126 & 0.088 & 0.088 & \textbf{0.107} & 0.082 \\ \hline
F1 score & 0.106 & 0.160 & 0.090 & 0.256 & 0.191 & 0.257 & 0.175 & 0.217 & 0.171 & 0.098 & 0.091 & 0.128 & 0.163 \\ \hline
\end{tabular}%
}
    \caption{Results for Disease terms using Bag of words feature selection}
    \label{tab:resultdiseasecount}
\end{table}

\subsection{Neural Network Methods}
For the neural network method, two models were trained - (i) BiLSTM and (ii) CNN. The embedding dimension in both cases is 300, where the input sequences are kept as 200 for which the results are discussed. The models are also retrained for a sequence length of 150 and 250 to check whether the accuracy increases or decreases. The models are trained using the Adam optimizer~\cite{kingma2014adam}. 
The results of the models are presented in Table~\ref{tab:DLdiseases}. The number of epochs is kept high because of a large number of labels. 

\begin{table}[!htb]
\centering
\begin{tabular}{|c|c|c|}
\hline
 & Validation accuracy & Sequence Length \\ \hline
\textbf{BiLSTM} & \textit{\textbf{0.177}} & 200 \\ \hline
 & 0.168 & 150 \\ \hline
 & 0.099 & 250 \\ \hline
\textbf{CNN} & 0.31 & 200 \\ \hline
 & 0.283 & 150 \\ \hline
 & \textit{\textbf{0.335}} & 250 \\ \hline
\end{tabular}
\caption{Results for deep neural networks for predicting disease names}
\label{tab:DLdiseases}
\end{table}

From Table~\ref{tab:bestdiseaseterms}, it can be seen that the CNN outperforms all the baseline methods and the BiLSTM. The BiLSTM has a lesser accuracy than the highest baseline MLP classifier. One explanation would be that since the RNN treats the input as a sequence, and its a very long sequence, as it goes ahead working on word after word, it forgets what happened in the words before. Even though we're using a bidirectional RNN, maybe the left to right forgets what happened at the start, say by the time it reaches mid sequence, and the right to left forgets what it saw in the first (rightmost) terms by the time it reaches mid sequence too. 

Whereas in CNNs, the convolution kernels are time invariant so they cannot distinguish between different parts of the sequence. As a disadvantage, they cannot easily do inferences which require using context and need to treat the input as a sequence. However, in this case, it might not be needed. Abstracts may somewhere mention a particular disease keyword which is enough on its own to detect which class it belongs to (and this detection requires no sequential treatment of input, which is similar to `find a word').

\begin{table}[!htb]
    \centering
    \begin{tabular}{|c|c|}
\hline
\textbf{Method} & \textbf{Accuracy} \\ \hline
\begin{tabular}[c]{@{}c@{}}MLP Classifier\\ (TF-IDF)\end{tabular} & 0.189 \\ \hline
\begin{tabular}[c]{@{}c@{}}MLP Classifier\\  (BoW)\end{tabular} & 0.157 \\ \hline
\begin{tabular}[c]{@{}c@{}}CNN\\ (length = 250)\end{tabular} & \textbf{0.335} \\ \hline
\begin{tabular}[c]{@{}c@{}}BiLSTM\\ (length = 200)\end{tabular} & 0.177 \\ \hline
\end{tabular}
    \caption{Best performing methods when predicting disease terms}
    \label{tab:bestdiseaseterms}
\end{table}

Moreover, we see from Table~\ref{tab:DLdiseases} that when the sequence length increases to 250 the accuracy for CNN increase, this can be because the disease terms would be occurring towards the end of the abstract. For further checking this hypothesis we perform experiments when we only consider some part of the abstract (see Table~\ref{tab:DTsentences}). For this, the abstracts are split into sentences and only a subset of these sentences are used per abstract. 

\begin{table}[!htb]
    \centering
\begin{tabular}{|c|c|c|}
\hline
 & Validation Accuracy & Sentences of abstract used \\ \hline
\textbf{BiLSTM} & 0.074 & First two \\ \hline
 & 0.073 & Last two \\ \hline
 & \textbf{\textit{0.103}} & First and last \\ \hline
\textbf{CNN} & 0.268 & First two \\ \hline
 & 0.133 & Last two \\ \hline
 & \textbf{\textit{0.28}} & First and last \\ \hline
\end{tabular}
    \caption{Results for predicting disease terms using deep neural nets when only some part of abstract is taken as input}
    \label{tab:DTsentences}
\end{table}

From Table~\ref{tab:DTsentences} we see that the accuracy does not increase when we use the last two sentences as we mentioned above. Therefore, such a generalization is not correct. Another interesting observation, in this case, is that both BiLSTM and CNN have higher accuracy when the first and last sentence are present in the abstract. This would be due to the fact that human beings read and write the first and last sentence very carefully with a lot of information which aligns to the myth about how humans read and write. Moreover, it adheres to how humans do fast reading.

%as we tend to focus mainly on the first and the last sentences more so researchers make an effort to write their first and last sentences to be crisp and full of information

\section{Predicting Super Classes}
%In this experiment we predict the superclasses (or parent classes) of the disease terms that are annotated in the corpus. 
 
In this experiment we use the MeSH tree structure\footnote{\url{https://meshb.nlm.nih.gov/treeView}} to map the annotated disease names to their superclasses (or parent classes) of the disease terms as shown in Figure~\ref{fig:meshsuperclass}. 
Since the dataset we use involves the disease terms from the Medical Subject Headings (MeSH) ontology - which provides hierarchically-organized terminology for indexing diseases. For example the disease term ``Hemochromatosis'' belongs to the superclass `Nutritional and Metabolic Diseases [C18]' and `Congenital, Hereditary, and Neonatal Diseases and Abnormalities [C16]'. 
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.4]{Figures/NN_prediction.png}
    \caption{Mapping superclasses in MeSH}
    \label{fig:meshsuperclass}
\end{figure}
To do so we use the unique ID of the disease terms as explained in Section~\ref{section:data} to perform a SPARQL query to map these terms (refer Listing~\ref{superclassquery}). 
%The motivation behind this experimental setup is to reduce the number of labels that the method has to predict. 
%By predicting super classes we want to depict a useful level of detail - the disease names are very detailed and there may be only a single example of a disease in the dataset.  The superclasses are much more general, so they abstract away from the specific disease to predict a group of related diseases which may be more useful in some contexts. 

Originally we want to map the disease terms to the \textbf{26} superclasses as depicted in Figure~\ref{fig:meshsuperclass}, however, the query returns 48 classes. 
%The classes ``Disorders of Environmental Origin [C21], Animal Diseases [C22]'' are not identified as superclasses for any diseases. 
The additional 24 additional classes that are returned as a result of the query (Listing~\ref{superclassquery}) are mentioned below:
\begin{spverbatim}
`Physical Phenomena', `Genetic Phenomena', `Population Characteristics', `Nonsyndromic sensorineural hearing loss', `Diagnosis', `Physiological Phenomena', `Psychological Phenomena', `Cell Physiological Phenomena', `Investigative Techniques', `Biological Phenomena', `Behavior and Behavior Mechanisms', `Immune System Phenomena', `Reproductive and Urinary Physiological Phenomena', `Fluids and Secretions', `Cells', `Health Occupations', `Environment and Public Health', `Tissues', `Mental Disorders', `Behavioral Disciplines and Activities', `Musculoskeletal and Neural Physiological Phenomena', `Therapeutics', `Health Care Quality, Access, and Evaluation', `Natural Science Disciplines'. 
\end{spverbatim}
This happens due to the fact that one disease term is sometimes present under more than one parent class. 
For example the terms `Genetic Phenomena (ID: [G05])' and `Pathological Conditions, Signs and Symptoms [C23]' are superclasses itself for the disease name `Chromosome Aberrations' as can be seen in \url{https://meshb.nlm.nih.gov/record/ui?ui=D002869}. 
%For example: `Nonsyndromic sensorineural hearing loss'  this was not annotated to ``preferred label", which is why query couldn't map to the DOI of the superclass.

\begin{lstlisting}[caption = SPARQL query to map \emph{one} disease name to the respective super class using the MeSH SPARQL endpoint (\url{https://id.nlm.nih.gov/mesh/query}), label = superclassquery]
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX owl: <http://www.w3.org/2002/07/owl#>
PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#>
PREFIX mesh: <http://id.nlm.nih.gov/mesh/>
PREFIX mesh2015: <http://id.nlm.nih.gov/mesh/2015/>
PREFIX mesh2016: <http://id.nlm.nih.gov/mesh/2016/>
PREFIX mesh2017: <http://id.nlm.nih.gov/mesh/2017/>
PREFIX mesh2018: <http://id.nlm.nih.gov/mesh/2018/>
PREFIX mesh2019: <http://id.nlm.nih.gov/mesh/2019/>
SELECT DISTINCT ?p ?label ?uri 
    WHERE { mesh:D012878 meshv:broaderDescriptor* ?uri .
            ?uri rdfs:label ?p.
            FILTER NOT EXISTS{
            ?uri meshv:broaderDescriptor ?x
        }
    }
\end{lstlisting}
As a result of extracting the super classes for disease terms, the number of labels is reduced to 48. In this experiment we perform text classification for these 48 labels.

\subsection{Multi-label Classification}
The results for the baselines are present in Table~\ref{tab:resulttfidf}, here the features were extracted using TF-IDF. Another set of features were extracted using the bag of word representation, the results are shown in the Table~\ref{tab:resultsBOW}. The maximum number of features for both the feature extraction techniques is kept to be 200. 
The Label Powerset transformation gives better performance for all three baseline classifiers. Overall, the Random forest classifier has the highest accuracy with problem transformation of Label Powerset. 

In this setup, the feature extraction using the bag of words technique has a better performance when compared to TF-IDF. An explanation behind this would be that in a random forest classification method when given a set of features and labels it, creates random subsets of features. Then building decision trees with the help of these subsets after which it makes predictions. 

%For MLP classifier the obtained are shown in the Table~\ref{table:MLP}. It can be seen that the classifier performs very poorly and the accuracy is 0 for the transformations of Binary Relevance and classifier chains. 

\begin{table}[!htb]
\centering
% \hskip-2.2cm
\resizebox{\textwidth}{!}{\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
Classifier & \multicolumn{3}{c|}{Random forest} & \multicolumn{3}{c|}{Decision Tree} & \multicolumn{3}{c|}{MLPClassifier} & \multicolumn{4}{c|}{MLkNN} \\ \hline
\begin{tabular}[c]{@{}c@{}}Problem\\ Transformation\end{tabular} & \multicolumn{1}{c|}{BR} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c|}{BR} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c|}{BR} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c|}{k=20} & \multicolumn{1}{c|}{k=30} & \multicolumn{1}{c|}{k=10} & \multicolumn{1}{c|}{k=5} \\ \hline
Accuracy & 0.181 & \textbf{0.348} & 0.174 & 0.148 & \textbf{0.271} & 0.135 & 0.232 & \textbf{0.348} & 0.335 & 0.303 & 0.284 & \textbf{0.316} & 0.297 \\ \hline
F1 score & 0.605 & 0.642 & 0.572 & 0.676 & 0.632 & 0.662 & 0.669 & 0.712 & 0.668 & 0.670 & 0.669 & 0.681 & 0.683 \\ \hline
\end{tabular}}
    \caption{Results of predicting super class using TF-IDF feature selection}
    \label{tab:resulttfidf}
\end{table}

\begin{table}[!htb]
    \centering
    \resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
 & \multicolumn{3}{l|}{Random forest} & \multicolumn{3}{l|}{Decision Tree} & \multicolumn{3}{l|}{MLPClassifier} & \multicolumn{4}{c|}{MLkNN} \\ \cline{2-14} 
 & \multicolumn{1}{c|}{BR} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c|}{BR} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c|}{BR} & \multicolumn{1}{c|}{LP} & \multicolumn{1}{c|}{CC} & \multicolumn{1}{c|}{k=20} & \multicolumn{1}{c|}{k=30} & \multicolumn{1}{c|}{k=10} & \multicolumn{1}{c|}{k=5} \\ \hline
\multicolumn{1}{|c|}{Accuracy} & 0.168 & \textbf{0.368} & 0.200 & 0.116 & \textbf{0.329} & 0.168 & 0.258 & \textbf{0.342} & 0.303 & \textbf{0.148} & 0.097 & 0.142 & 0.142 \\ \hline
\multicolumn{1}{|c|}{F1 score} & 0.605 & 0.649 & 0.554 & 0.641 & 0.657 & 0.630 & 0.668 & 0.672 & 0.674 & 0.520 & 0.510 & 0.521 & 0.531 \\ \hline
\end{tabular}%
}
    \caption{Results of predicting super class using Bag of words feature selection}
    \label{tab:resultsBOW}
\end{table}

\subsection{Neural Network Methods}
For the neural network method, two models were trained - (i) BiLSTM and (ii)
CNN. The dimension of the embedding matrix in both cases is 300, where the input sequences of the abstract
are kept as 200 for which the results are discussed. The models are also retrained
for a sequence length of 150 and 250 to check whether the accuracy increases or
decreases. The models are trained using the Adam optimizer~\cite{kingma2014adam}. The results are shown in the Table~\ref{tab:DLsuperclass}. The number of epochs is kept to 65 as the number of labels is not high as in the previous setup. 

\begin{table}[!htb]
    \centering
\begin{tabular}{|l|c|c|}
\hline
\multicolumn{1}{|c|}{} & \textbf{\begin{tabular}[c]{@{}c@{}}Validation \\ Accuracy\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Sequence\\  Length\end{tabular}} \\ \hline
\textbf{BiLSTM} & 0.623 & 200 \\ \hline
 & 0.628 & 150 \\ \hline
 & \textit{\textbf{0.697}} & 250 \\ \hline
\textbf{CNN} & \textit{\textbf{0.837}} & 200 \\ \hline
 & 0.76 & 150 \\ \hline
 & 0.803 & 250 \\ \hline
\end{tabular}
    \caption{Results for deep neural networks for predicting super class}
    \label{tab:DLsuperclass}
\end{table}

Here both the BiLSTM and CNN outperform all the baseline classifiers. Overall, CNN has the highest performance accuracy with the sequence length of 200. Another interesting observation is that when the sequence length is increased to 250 the accuracy of the BiLSTM increases, whereas one would think it should decrease because LSTMs tend to forget what happened previously or ahead when the sequence length is increased. 
One explanation behind this could be that the disease words occur towards the end in the abstract. When we use the last two sentences per abstract for predicting the superclass in BiLSTM the accuracy decreases to 48.4\% (see Table~\ref{tab:Supersentences}). Therefore, a generalization of such kind cannot be made. Similar to predicting disease terms here also the model performs better when the first and last sentence is taken as the input. 

\begin{table}[!htb]
    \centering
    \begin{tabular}{|c|c|}
\hline
\textbf{Method} & \textbf{Accuracy} \\ \hline
\begin{tabular}[c]{@{}c@{}}Random Forest and MLP Classifier\\ (TF-IDF)\end{tabular} & 0.348 \\ \hline
\begin{tabular}[c]{@{}c@{}}Random Forest\\ (BoW)\end{tabular} & 0.368 \\ \hline
\begin{tabular}[c]{@{}c@{}}CNN\\ (length = 200)\end{tabular} & \textbf{0.837} \\ \hline
\begin{tabular}[c]{@{}c@{}}BiLSTM\\ (length = 250)\end{tabular} & 0.697 \\ \hline
\end{tabular}
    \caption{Best performance among the different methods when predicting super classes}
    \label{tab:bestsuperclass}
\end{table}

\begin{table}[!htb]
    \centering
    \begin{tabular}{|c|c|c|}
\hline
 & Validation Accuracy & Sentences of abstract used \\ \hline
\textbf{BiLSTM} & 0.518 & First two \\ \hline
 & 0.484 & Last two \\ \hline
 & \textbf{0.60} & First and last \\ \hline
\textbf{CNN} & 0.76 & First two \\ \hline
 & 0.771 & Last two \\ \hline
 & \textbf{0.787} & First and last \\ \hline
\end{tabular}
    \caption{Results for predicting super classes using deep neural nets when only some part of abstract is taken as input}
    \label{tab:Supersentences}
\end{table}

\section{Discussion}\label{sec:discussion}
In this section, we mainly discuss the results that we obtain using the neural network method, specifically for the sequence length of 200. The reason why we keep this as a gold standard input length is that it is closest to the mean length of the number of words in all the abstracts (refer Section~\ref{section:data}). Furthermore, we also observed from the results that by increasing the sequence length the performance does not significantly increase. Therefore, rather than training at a maximum length we use the mean length as it also saves the running time of the experiments.    

For this discussion, we pick three abstracts that are common in the validation set of both the disease term and superclass experiment. Note that the text is preprocessed. 

\newenvironment{boxet}
    {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    }
    { 
    \\\\\hline
    \end{tabular} 
    \end{center}
    }
    %--------------------------------------------------
\begin{boxet}
Abstract 1: \textsf{promoter luciferase constructs were transiently cotransfected with a wild type vhl wt vhl vector in several cell lines including 293 embryonic kidney and rcc cell lines wt vhl protein inhibited vegf promoter activity in a dose dependent manner up to 5 to 10 fold deletion analysis defined a 144 bp region of the vegf promoter necessary for vhl repression this vhl responsive element is gc rich and specifically binds the transcription factor sp1 in crude nuclear extracts in drosophila cells cotransfected vhl represses sp1 mediated activation but not basal activity of the vegf promoter we next demonstrated in coimmunoprecipitates that vhl and sp1 were part of the same complex and by using a glutathione s transferase vhl fusion protein and purified sp1 that vhl and sp1 directly interact furthermore endogenous vegf mrna levels were suppressed in permanent rcc cell lines expressing wt vhl and nuclear run on studies indicated that vhl regulation of vegf occurs at least partly at the transcriptional level these observations support a new mechanism for vhl mediated transcriptional repression via a direct inhibitory action on sp1 and suggest that loss of sp1 inhibition may be important in the pathogenesis of von hippel lindau disease and rcc.}

Actual labels: 

\emph{Superclass} - [`Congenital, Hereditary, and Neonatal Diseases and Abnormalities', `Nervous System Diseases', `Cardiovascular Diseases', `Male Urogenital Diseases', `Female Urogenital Diseases and Pregnancy Complications', `Skin and Connective Tissue Diseases', `Neoplasms']

\emph{Disease Names} - [`von Hippel-Lindau Disease', `Carcinoma, Renal Cell'].
\end{boxet}
\begin{boxet}
Abstract 2: \textsf{we have analyzed the 27 exons and the promoter region of the rb1 gene in familial or sporadic bilateral retinoblastoma by using single strand conformation polymorphism analysis for improvement over previous studies a new set of primers has been designed which allow for amplification of the coding and splicing sequences only the positioning of the polymerase chain reaction pcr primers was such that the resulting pcr products were of different sizes which enabled us to analyze two different exons simultaneously and still distinguish between the banding profiles for both biplex analysis by using this approach we were able to identify mutation in 22 new patients but the overall efficiency of the procedure when we used a single pass regimen was only 48 the mutations were small insertions and deletions and point mutations in roughly equal proportions}

Actual labels:\emph{Super class} -  [`Congenital, Hereditary, and Neonatal Diseases and Abnormalities', `Neoplasms', `Eye Diseases']

\emph{Disease names} - [`Retinoblastoma']
\end{boxet}
\begin{boxet}
Abstract 3: \textsf{programmed cell death or apoptosis is a physiological process essential to the normal development and homeostatic maintenance of the immune system the fas apo 1 receptor plays a crucial role in the regulation of apoptosis as demonstrated by lymphoproliferation in mrl lpr lpr mice and by the recently described autoimmune lymphoproliferative syndrome alps in humans both of which are due to mutations in the fas gene we describe a novel family with alps in which three affected siblings carry two distinct missense mutations on both the fas gene alleles and show lack of fas induced apoptosis the children share common clinical features including splenomegaly and lymphadenopathy but only one developed severe autoimmune manifestations in all three siblings we demonstrated the presence of anergic cd3 cd4 cd8 double negative dn t cells moreover a chronic lymphocyte activation was found as demonstrated by the presence of high levels of hla dr expression on peripheral cd3 cells and by the presence of high levels of serum activation markers such as soluble interleukin 2 receptor sll 2r and soluble cd30 scd30}

Actual Label: \emph{Super class} - [`Congenital, Hereditary, and Neonatal Diseases and Abnormalities', `Pathological Conditions, Signs and Symptoms', `Immune System Diseases', `Hemic and Lymphatic Diseases']

\emph{Disease Names} - [`Autoimmune Lymphoproliferative Syndrome', `Splenomegaly', `Lymphatic Diseases', `Autoimmune Diseases']

\end{boxet}

\begin{table}[!htb]
    \centering
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{} & \multicolumn{2}{c|}{CNN} \\ \hline
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Predicted \\ Disease Names\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Predicted \\ Super Classes\end{tabular}} \\ \hline
Abstract 1 &  & \begin{tabular}[c]{@{}l@{}}`Cardiovascular Diseases',\\  `Female Urogenital Diseases and Pregnancy Complications', \\  `Nervous System Diseases', `Skin and Connective Tissue Diseases', \\ `Nutritional and Metabolic Diseases', `Neoplasms', \\ `Congenital, Hereditary, and Neonatal Diseases and Abnormalities'\end{tabular} \\ \hline
Abstract 2 & `Retinoblastoma' & \begin{tabular}[c]{@{}l@{}}`Congenital, Hereditary, and Neonatal Diseases and Abnormalities', \\ `Neoplasms', `Eye Diseases\end{tabular} \\ \hline
Abstract 3 &  & `Congenital, Hereditary, and Neonatal Diseases and Abnormalities' \\ \hline
\end{tabular}
    \caption{Prediction by CNN for the three abstracts}
    \label{tab:predictions_cnn}
\end{table}

\begin{table}[!htb]
    \centering
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{} & \multicolumn{2}{c|}{BiLSTM} \\ \hline
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Predicted\\ Disease Name\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Predicted\\ Super Class\end{tabular}} \\ \hline
Abstract 1 & `Neoplasms' & \begin{tabular}[c]{@{}l@{}}`Cardiovascular Diseases', `Neoplasms', \\ `Skin and Connective Tissue Diseases'\end{tabular} \\ \hline
Abstract 2 &  & \begin{tabular}[c]{@{}l@{}}`Neoplasms', `Digestive System Diseases', \\ `Congenital, Hereditary, and Neonatal Diseases and Abnormalities', \\ `Skin and Connective Tissue Diseases'\end{tabular} \\ \hline
Abstract 3 &  & \begin{tabular}[c]{@{}l@{}}`Immune System Diseases', `Hemic and Lymphatic Diseases', \\ `Congenital, Hereditary, and Neonatal Diseases and Abnormalities', \\ `Cardiovascular Diseases', `Nutritional and Metabolic Diseases'\end{tabular} \\ \hline
\end{tabular}
    \caption{Prediction by BiLSTM for the three abstracts}
    \label{tab:preidictions_lstm}
\end{table}

The predictions of the three abstracts are presented in Tables~\ref{tab:predictions_cnn} and~\ref{tab:preidictions_lstm}. From the predictions, it can be seen when we use a large number of labels(588), in $\approx38\%$ abstracts in the validation set there is no prediction at all for both CNN and BiLSTM. For this, we need a bigger dataset for the models to train on. If we get a larger annotated set, the model will be able to learn in a much better way as it would have seen a good amount of examples for such a large set of labels.  

We observed that when predicting super classes CNN is very accurate(83\%) as it would have seen enough examples to learn which label would fit where. Similarly, BiLSTM is accurate more than 50\% of times overall even in some cases where CNN isn't, for example in Abstract 3. This could be because in this case the context of the language would be required more rather than in the other cases. For example, the abstract 3 talks about immune system diseases so in this case, the BiLSTM learns this whereas CNN would not be able to do that.  
In some cases, no result comes because the last layer is the softmax layer and the probability of each category would be less than 0.5, so the model does not output any disease.

For evaluating whether there is any bias present in the predictions for superclasses, we perform an analysis of the frequency of the labels. First we compute the frequency of each label for the whole dataset, then specifically for the validation set. This is done for both the CNN and BiLSTM. 
From the analysis, it was observed that the superclass ``Congenital, Hereditary, and Neonatal Diseases and Abnormalities'' has the highest frequency of occurrence (639). Therefore, even in the validation set for CNN and for BiLSTM both, it has the highest frequency. Note that in the validation set, the frequency is calculated separately for predictions and the actual labels. In the predictions, the frequency was 139 whereas in the actual labels it occurs 125 times for BiLSTM. Similarly, for CNN the frequency in actual labels was 144 whereas the frequency is 126 in the predicted labels. From the frequency, we observe that the model predicts better if the label has a higher number of occurrences. 

We perform a similar analysis for the less frequently occurring labels: ``Bacterial Infections and Mycoses''. This label has a frequency of 23 in the entire label set. After looking into the validation set for both CNN and BiLSTM, we observed that in CNN it is predicted 7 times but actually it occurs only once. For BiLSTM, it is predicted only once, but in actual labels, it has a frequency of 7. Since the overall occurrence of ``Bacterial Infections and Mycoses'' is not higher the predictions are not as accurate as we observe before with the label ``Congenital, Hereditary, and Neonatal Diseases and Abnormalities''.  

From this analysis, we see that a large set of disease names has one particular, therefore, there is a bias when we try to predict these superclasses. Moreover, the input corpus contains 793 abstracts in total, which only gives a particular subset of diseases. Hence there is a bias in prediction related to the frequency of occurrences when extracting the superclasses. From the results, we saw that CNN gives promising results in both experimental setups -- predicting disease terms and predicting superclasses. If we want to involve more metadata categories like tissue, cell line etc, it would be easily incorporated in the model as we train a multi-label classification problem. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% commented out section - don't read %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{table}[!htb]
\begin{tabular}{|l|c|c|c|}
\hline
MLP Classifier           & \multicolumn{1}{l|}{Binary Relevance} & \multicolumn{1}{l|}{Label Powerset} & \multicolumn{1}{l|}{Classifier Chains} \\ \hline
Accuracy                 & 0.232                                 & 0.348                               & 0.335                                  \\ \hline
Hamming Loss             & 0.050                                 & 0.044                               & 0.049                                  \\ \hline
Weighted F1 score        & 0.669                                 & 0.712                               & 0.668                                  \\ \hline
Weighted Precision score & 0.712                                 & 0.799                               & 0.734                                  \\ \hline
\end{tabular}
\caption{Results for MLP classifier}
\label{table:MLP}
\end{table}




\begin{table}[!htb]
    \centering
    \begin{tabular}{|l|c|c|c|}
\hline
Random Forest Classifier & \multicolumn{1}{l|}{Binary Relevance} & \multicolumn{1}{l|}{Label Powerset} & \multicolumn{1}{l|}{Classifier Chains} \\ \hline
Accuracy                 & 0.181                                 & 0.348                               & 0.174                                  \\ \hline
Hamming Loss             & 0.050                                 & 0.058                               & 0.052                                  \\ \hline
Weighted F1 score        & 0.605                                 & 0.642                               & 0.572                                  \\ \hline
Weighted Precision score & 0.804                                 & 0.711                               & 0.823                                  \\ \hline
\end{tabular}
    \caption{Results for Random Forest Classifier}
    \label{tab:randomforest}
\end{table}

\begin{table}[!htb]
    \centering
\begin{tabular}{|l|c|c|c|}
\hline
Decision Tree Classifier    & \multicolumn{1}{l|}{Binary Relevance} & \multicolumn{1}{l|}{Label Powerset} & \multicolumn{1}{l|}{Classifier Chains} \\ \hline
Accuracy                 & 0.148                                 & 0.271                               & 0.135                                  \\ \hline
Hamming Loss             & 0.059                                 & 0.068                               & 0.063                                  \\ \hline
Weighted F1 score        & 0.676                                 & 0.632                               & 0.662                                  \\ \hline
Weighted Precision score & 0.686                                 & 0.632                               & 0.681                                  \\ \hline
\end{tabular}
    \caption{Results for Decision Tree Classifier}
    \label{tab:decisiontree}
\end{table}

\begin{table}[!htb]
    \centering
    \begin{tabular}{|l|c|c|c|l|}
\hline
MLkNN                    & \multicolumn{1}{l|}{k=20} & \multicolumn{1}{l|}{k=30} & \multicolumn{1}{l|}{k=10} & k=5   \\ \hline
Accuracy                 & 0.303                     & 0.284                     & 0.316                     & 0.297 \\ \hline
Hamming Loss             & 0.045                     & 0.044                     & 0.044                     & 0.045 \\ \hline
Weighted F1 score        & 0.670                     & 0.669                     & 0.681                     & 0.683 \\ \hline
Weighted Precision score & 0.788                     & 0.802                     & 0.783                     & 0.821 \\ \hline
\end{tabular}
    \caption{Results for kNN adaptation for Multilabel Classfication}
    \label{tab:MLkNN}
\end{table}
\begin{table}[!htb]
    \centering
    \begin{tabular}{|l|c|c|c|}
\hline
Gradient Boost Classifier & \multicolumn{1}{l|}{Binary Relevance} & \multicolumn{1}{l|}{Label Powerset} & \multicolumn{1}{l|}{Classifier Chains} \\ \hline
Accuracy                   & 0.0                                   & 0.019                               & 0.006                                  \\ \hline
Hamming Loss               & 0.114                                 & 0.133                               & 0.112                                  \\ \hline
Weighted F1 score          & 0.3                                   & 0.349                               & 0.3                                    \\ \hline
Weighted Precision score   & 0.319                                 & 0.354                               & 0.345                                  \\ \hline
\end{tabular}
    \caption{Results for Gradient Boost Classifier}
    \label{tab:gradientboost}
\end{table}

\begin{table}[!htb]
    \centering
    \begin{tabular}{|l|c|c|c|}
\hline
Linear SVC Classifier    & \multicolumn{1}{l|}{Binary Relevance} & \multicolumn{1}{l|}{Label Powerset} & \multicolumn{1}{l|}{Classifier Chains} \\ \hline
Accuracy                 & 0.0                                   & 0.0452                              & 0.0                                    \\ \hline
Hamming Loss             & 0.152                                 & 0.108                               & 0.147                                  \\ \hline
Weighted F1 score        & 0.375                                 & 0.321                               & 0.378                                  \\ \hline
Weighted Precision score & 0.369                                 & 0.335                               & 0.365                                  \\ \hline
\end{tabular}
    \caption{Results for Linear SVC Classifier}
    \label{tab:linearsvc}
\end{table}


\section{Baseline Method}
In the basic method which involves prediction of metadata, specifically diseases. A baseline method would be to use a vocabulary of disease names and perform a basic search in the documents. If we have a match it can be said that a particular publication talks about `xyz' disease. This would be a direct and trivial way to approach this problem. 

In this case, since the dataset uses the MeSH ontology, so all the values of disease names are downloaded\footnote{\url{http://bioportal.bioontology.org/ontologies/MESH/?p=classes&conceptid=root}}. After that, a direct matching is done using Flashtext~\cite{2017arXiv171100046S}. Firstly the downloaded diseases from the MeSH contains a lot of secondary information which is not essential, therefore columns with disease names are selected. This also includes the synonyms so that the matching is not biased for terms like `heart disease' and `cardiovascular disease'. Even after including the synonyms we could find only $44$ unique terms in the text, which is $\approx$2\% of the diseases identified disease terms. 
%write the results

\section{Predicting Super class}
Prediction of super class means using the tree hierarchy the Mesh ontology, and find the superclass of all the disease terms~\footnote{\url{https://meshb.nlm.nih.gov/treeView}}. In the superclass of diseases we only have 26 targets, then it becomes fairly easy for the model to predict one of these 26 classes. 



\section{Predicting the disease terms}
Predicting disease class involves, selecting the disease terms that were annotated to the Disease Class (Refer Table~\ref{table: input_stats}). From the Table~\ref{table: input_stats} we see that the number of diseases, in this case, would be 571, which is more difficult than identifying superclasses. 

\section{Discussion}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% DON'T READ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%